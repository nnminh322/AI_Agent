#llm/configs.yaml
llm:
  # mode: host
  model: "Qwen/Qwen3-4B-Instruct-2507"
  # model: "Qwen/Qwen3-8B"
  temperature: 0.6
  top_p: 0.8
  max_tokens: 512
  tensor_parallel_size: 2
  max_model_len: 4096
  dtype: "half"
  
resource:
  gpu_memory_utilization: 0.8
  port: 8000
  host: "127.0.0.1"
  swap_space: 8
