#llm/configs.yaml
llm:
  # mode: host
  model: "Qwen/Qwen3-VL-8B-Instruct"
  temperature: 0.85
  top_p: 0.95
  max_tokens: 512
  tensor_parallel_size: 1
  max_model_len: 4096
  dtype: "half"
  
resource:
  gpu_memory_utilization: 0.8
  port: 8000
  host: "127.0.0.1"
  swap_space: 16
